<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Introduction · ChainRules</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/><link href="assets/chainrules.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>ChainRules</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href="index.html">Introduction</a><ul class="internal"><li><a class="toctext" href="#Example-of-using-ChainRules-directly.-1">Example of using ChainRules directly.</a></li><li><a class="toctext" href="#On-writing-good-rrule-/-frule-methods-1">On writing good <code>rrule</code> / <code>frule</code> methods</a></li><li><a class="toctext" href="#FAQ-1">FAQ</a></li></ul></li><li><a class="toctext" href="getting_started.html">Getting Started</a></li><li><a class="toctext" href="api.html">API</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href="index.html">Introduction</a></li></ul><a class="edit-page" href="https://github.com/JuliaDiff/ChainRules.jl/blob/master/docs/src/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Introduction</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="ChainRules-1" href="#ChainRules-1">ChainRules</a></h1><p><a href="https://github.com/JuliaDiff/ChainRules.jl">ChainRules</a> provides a variety of common utilities that can be used by downstream <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation (AD)</a> tools to define and execute forward-, reverse-, and mixed-mode primitives.</p><h3><a class="nav-anchor" id="Introduction-1" href="#Introduction-1">Introduction</a></h3><p>ChainRules is all about providing a rich set of rules for differentiation. When a person learns introductory calculus, they learn that the derivative (with respect to <code>x</code>) of <code>a*x</code> is <code>a</code>, and the derivative of <code>sin(x)</code> is <code>cos(x)</code>, etc. And they learn how to combine simple rules, via <a href="https://en.wikipedia.org/wiki/Chain_rule">the chain rule</a>, to differentiate complicated functions. ChainRules is a programmatic repository of that knowledge, with the generalizations to higher dimensions.</p><p><a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Autodiff (AD)</a> tools roughly work by reducing a problem down to simple parts that they know the rules for, and then combining those rules. Knowing rules for more complicated functions speeds up the autodiff process as it doesn&#39;t have to break things down as much.</p><p><strong>ChainRules is an AD-independent collection of rules to use in a differentiation system.</strong></p><h3><a class="nav-anchor" id="frule-and-rrule-1" href="#frule-and-rrule-1"><code>frule</code> and <code>rrule</code></a></h3><div class="admonition terminology"><div class="admonition-title">`frule` and `rrule`</div><div class="admonition-text"><p><code>frule</code> and <code>rrule</code> are ChainRules specific terms. Their exact functioning is fairly ChainRules specific, though other tools have similar functions. The core notion is sometimes called <em>custom AD primitives</em>, <em>custom adjoints</em>, <em>custom</em>gradients<em>, _custom sensitivities</em>. (Potentially incorrectly, terminology is often abused.)</p></div></div><p>The rules are encoded as <code>frule</code>s and <code>rrule</code>s, for use in forward-mode and reverse-mode differentiation respectively.</p><p>Similarly, the <code>frule</code> is written:</p><pre><code class="language-julia">function frule(::typeof(foo), args; kwargs...)
    ...
    return y, pushforward
end</code></pre><p>where <code>y = foo(args; kwargs...)</code>, and <code>pushforward</code> is a function to propagate the derivative information forwards at that point (more later).</p><p>The <code>rrule</code> for some function <code>foo</code>, which takes the positional argument <code>args</code> and keyword argument <code>kwargs</code>, is written:</p><pre><code class="language-julia">function rrule(::typeof(foo), args; kwargs...)
    ...
    return y, pullback
end</code></pre><p>again <code>y</code> must be equal to <code>foo(args; kwargs...)</code>, and <code>pullback</code> is a function to propagate the derivative information backwards at that point (more later).</p><p>Almost always the <em>pushforward</em>/<em>pullback</em> will be declared locally within the <code>frule</code>/<code>rrule</code>, and will be a <em>closure</em> over some of the other arguments.</p><h3><a class="nav-anchor" id="The-propagators:-pushforward-and-pullback-1" href="#The-propagators:-pushforward-and-pullback-1">The propagators: pushforward and pullback</a></h3><div class="admonition terminology"><div class="admonition-title">pushforward and pullback</div><div class="admonition-text"><p><em>Pushforward</em> and <em>pullback</em> are fancy words that the autodiff community adopted from Differential Geometry. The are broadly in agreement with the use of <a href="https://en.wikipedia.org/wiki/Pullback_(differential_geometry)">pullback</a> and <a href="https://en.wikipedia.org/wiki/Pushforward_(differential)">pushforward</a> in differential geometry. But any geometer will tell you these are the super-boring flat cases. Some will also frown at you. Other terms that may be used include for <em>pullback</em> the <strong>backpropagator</strong>, and by analogy for <em>pushforward</em> the <strong>forwardpropagator</strong>, thus these are the <em>propagators</em>. These are also good names because effectively they propagate wiggles and wobbles through them, via the chain rule. (the term <strong>backpropagator</strong> may originate with <a href="http://www-bcl.cs.may.ie/~barak/papers/toplas-reverse.pdf">&quot;Lambda The Ultimate Backpropagator&quot;</a> by Pearlmutter and Siskind, 2008)</p></div></div><h4><a class="nav-anchor" id="Core-Idea-1" href="#Core-Idea-1">Core Idea</a></h4><hr/><h5><a class="nav-anchor" id="TODO:-Incorperate-this:-1" href="#TODO:-Incorperate-this:-1">TODO: Incorperate this:</a></h5><h6><a class="nav-anchor" id="wesselb-9-days-ago-Member-1" href="#wesselb-9-days-ago-Member-1">wesselb 9 days ago Member</a></h6><p>Are these ideas consistent with what pushforward and pullback do? I&#39;m not familiar with ChainRules and its internals, but I anticipated pushforward and pullback to do the following: Consider a computation x -&gt; u -&gt; f(u) = v -&gt; y. Then pushforward for f turns du/dx into dv/dx, whereas pullback turns dy/dv into dy/du. So pushforward pushes a &quot;sensitivity with respect to the input through the function&quot;, whereas pullback pulls a &quot;sensitivity with respect to the output back through the function&quot;. Perhaps that&#39;s what the below convey, not sure... maybe I&#39;m just rambling.</p><h6><a class="nav-anchor" id="@jekbradbury-1" href="#@jekbradbury-1">@jekbradbury</a></h6><p>Yeah, I think the below is accurate for the pushforward but misleading for the pullback. The pullback doesn’t take an output wobble and produce an input wiggle (that would be left-multiplying by the inverse of the Jacobian); it takes an output sensitivity (“how much does the loss function wobble when you wiggle the output”) and produces an input sensitivity (“how much does the loss function wobble when you wiggle the input”). This corresponds to left-multiplying by the adjoint of the Jacobian—an important distinction!</p><p>If the output is the scalar loss and you call the pullback on the scalar 1, then it will produce the gradient of the input (also a vector in the cotangent space, aka a wobble-wiggle ratio).</p><p>This is still misleading for the pullback. Reposting a comment that got lost: The pullback doesn’t take an output wobble and produce an input wiggle (that would be left-multiplying by the inverse of the Jacobian); it takes an output sensitivity (“how much does the loss function wobble when you wiggle the output”) and produces an input sensitivity (“how much does the loss function wobble when you wiggle the input”). This corresponds to left-multiplying by the adjoint of the Jacobian—an important distinction!</p><p>If the output is the scalar loss and you call the pullback on the scalar 1, then it will produce the gradient of the input (also a vector in the cotangent space, aka a wobble-wiggle ratio).</p><hr/><ul><li>The <strong>pushforward</strong> takes a wiggle in the <em>input space</em>, and tells what wobble you would create in the output space, by passing it through the function.</li><li>The <strong>pullback</strong> takes a wobble in the <em>output space</em>, and tells you what wiggle you would need to make in the <em>input space</em> to achieve it.</li></ul><h4><a class="nav-anchor" id="The-anatomy-of-pushforward-and-pullback-1" href="#The-anatomy-of-pushforward-and-pullback-1">The anatomy of pushforward and pullback</a></h4><p>For our function <code>foo(args...; kwargs) = Y</code>:</p><p>The pushforward is a function:</p><pre><code class="language-julia">function pushforward(Δself, Δargs...)
    ...
    return ∂Y
end</code></pre><p>The input to the pushforward is often called the <em>perturbation</em>. If the function is <code>y = f(x)</code> often the pushforward will be written <code>ẏ = pushforward(ṡelf, ẋ)</code>. (<code>ẏ</code> is commonly used to represent the pertubation for <code>y</code>)</p><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>There is one <code>Δarg</code> per <code>arg</code> to the original function. The <code>Δargs</code> are similar in type/structure to the corresponding inputs <code>args</code> (<code>Δself</code> is explained below). The <code>∂Y</code> are similar in type/structure to the original function&#39;s output <code>Y</code>. In particular if that function returned a tuple then <code>∂Y</code> will be a tuple of same size.</p></div></div><p>The pullback is a function:</p><pre><code class="language-julia">function pullback(ΔY)
    ...
    return ∂self, ∂args...
end</code></pre><p>The input to the pullback is often called the <em>seed</em>. If the function is <code>y = f(x)</code> often the pullback will be written <code>s̄elf, x̄ = pullback(ȳ)</code>.</p><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>The pullback returns one <code>∂arg</code> per <code>arg</code> to the original function, plus one for the fields of the function itself (explained below).</p></div></div><div class="admonition terminology"><div class="admonition-title">Terminology</div><div class="admonition-text"><p>Sometimes <em>perturbation</em>, <em>seed</em>, and <em>sensitivity</em> will be used interchangeably, depending on task/subfield (sensitivity analysis and perturbation theory are apparently very big on just calling everything <em>sensitivity</em> or <em>perturbation</em> respectively.) At the end of the day, they are all <em>wiggles</em> or <em>wobbles</em>.</p></div></div><h3><a class="nav-anchor" id="Self-derivative-Δself,-self,-self,-self-etc.-1" href="#Self-derivative-Δself,-self,-self,-self-etc.-1">Self derivative <code>Δself</code>, <code>∂self</code>, <code>s̄elf</code>, <code>ṡelf</code> etc.</a></h3><p>!!! terminology  <code>Δself</code>, <code>∂self</code>, <code>s̄elf</code>, <code>ṡelf</code>     It is the derivatives with respect to the internal fields of the function.     To the best of our knowledge there is no standard terminology for this.     Other good names might be <code>Δinternal</code>/<code>∂internal</code>.</p><p>From the mathematical perspective, one may have been wondering what all this <code>Δself</code>, <code>∂self</code> is. Given that a function with two inputs, say <code>f(a, b)</code>, only has two partial derivatives: <span>$\dfrac{∂f}{∂a}$</span>, <span>$\dfrac{∂f}{∂b}$</span>. Why then does a <code>pushforward</code> take in this extra <code>Δself</code>, and why does a <code>pullback</code> return this extra <code>∂self</code>?</p><p>The reason is that in Julia the function <code>f</code> may itself have internal fields. For example a closure has the fields it closes over; a callable object (i.e. a functor) like a <code>Flux.Dense</code> has the fields of that object.</p><p><strong>Thus every function is treated as having the extra implicit argument <code>self</code>, which captures those fields.</strong> So every <code>pushforward</code> takes in an extra argument, which is ignored unless the original function has fields. It is common to write <code>function foo_pushforward(_, Δargs...)</code> in the case when <code>foo</code> does not have fields. Similarly every <code>pullback</code> returns an extra <code>∂self</code>, which for things without fields is the constant <code>NO_FIELDS</code>, indicating there are no fields within the function itself.</p><h4><a class="nav-anchor" id="Pushforward-/-Pullback-summary-1" href="#Pushforward-/-Pullback-summary-1">Pushforward / Pullback summary</a></h4><ul><li><p><strong>Pushforward:</strong></p><ul><li>returned by <code>frule</code></li><li>takes input space wiggles, gives output space wobbles</li><li>1 argument per original function argument + 1 for the function itself</li><li>1 return per original function return</li></ul></li><li><p><strong>Pullback</strong></p><ul><li>returned by <code>rrule</code></li><li>takes output space wobbles, gives input space wiggles</li><li>1 argument per original function return</li><li>1 return per original function argument + 1 for the function itself</li></ul></li></ul><h4><a class="nav-anchor" id="Pushforward/Pullback-and-Total-Derivative/Gradient-1" href="#Pushforward/Pullback-and-Total-Derivative/Gradient-1">Pushforward/Pullback and Total Derivative/Gradient</a></h4><p>The most trivial use of <code>frule</code> and returned <code>pushforward</code> is to calculate the <a href="https://en.wikipedia.org/wiki/Total_derivative">Total Derivative</a>:</p><pre><code class="language-julia">y, f_pushforward = frule(f, a, b, c)
ẏ = f_pushforward(1, 1, 1, 1)  # for appropriate `1`-like perturbation.</code></pre><p>Then we have that <code>ẏ</code> is the <em>total derivative</em> of <code>f</code> at <code>(a, b, c)</code>, written mathematically as <span>$df_{(a,b,c)}$</span></p><p>Similarly, the most trivial use of <code>rrule</code> and returned <code>pullback</code> is to calculate the <a href="https://en.wikipedia.org/wiki/Gradient">Gradient</a>:</p><pre><code class="language-julia">y, f_pullback = rrule(f, a, b, c)
∇f = f_pullback(1)  # for appropriate `1`-like seed.
s̄elf, ā, b̄, c̄ = ∇f</code></pre><p>Then we have that <code>∇f</code> is the <em>gradient</em> of <code>f</code> at <code>(a, b, c)</code>. And we thus have the partial derivatives <span>$\overline{\mathrm{self}}, = \dfrac{∂f}{∂\mathrm{self}}$</span>, <span>$\overline{a} = \dfrac{∂f}{∂a}$</span>, <span>$\overline{b} = \dfrac{∂f}{∂b}$</span>, <span>$\overline{c} = \dfrac{∂f}{∂c}$</span>, including the and the self-partial derivative, <span>$\overline{\mathrm{self}}$</span>.</p><h3><a class="nav-anchor" id="Differentials-1" href="#Differentials-1">Differentials</a></h3><p>The values that come back from pullbacks or pushforwards are not always the same type as the input/outputs of the original function. They are differentials, which correspond roughly to something able to represent the difference between two values of the original types. A differential might be such a regular type, like a <code>Number</code>, or a <code>Matrix</code>, matching to the original type; or it might be one of the <code>AbstractDifferential</code> subtypes.</p><p>Differentials support a number of operations. Most importantly: <code>+</code> and <code>*</code>, which let them act as mathematical objects. And <code>extern</code> which converts <code>AbstractDifferential</code> types into a conventional non-ChainRules type.</p><p>The most important <code>AbstractDifferential</code>s when getting started are the ones about avoiding work:</p><ul><li><code>Thunk</code>: this is a deferred computation. A thunk is a <a href="https://en.wikipedia.org/wiki/Thunk">word for a zero argument closure</a>. A computation wrapped in a <code>@thunk</code> doesn&#39;t get evaluated until <code>extern</code> is called on the <code>Thunk</code>. More on thunks later.</li><li><code>One</code>, <code>Zero</code>: There are special representations of <code>1</code> and <code>0</code>. They do great things around avoiding expanding <code>Thunks</code> in multiplication and (for <code>Zero</code>) addition.</li></ul><h4><a class="nav-anchor" id="Other-AbstractDifferentials:-don&#39;t-worry-about-them-right-now-1" href="#Other-AbstractDifferentials:-don&#39;t-worry-about-them-right-now-1">Other <code>AbstractDifferential</code>s: don&#39;t worry about them right now</a></h4><ul><li><code>Wirtinger</code>: it is complex. The docs need to be better. <a href="https://github.com/JuliaDiff/ChainRulesCore.jl/issues/40">Read the links in this issue</a>.</li><li><code>Casted</code>: it implements broadcasting mechanics. See <a href="https://github.com/JuliaDiff/ChainRulesCore.jl/issues/10">#10</a></li><li><code>InplaceableThunk</code>: it is like a Thunk but it can do <code>store!</code> and <code>accumulate!</code> in-place.</li></ul><hr/><h2><a class="nav-anchor" id="Example-of-using-ChainRules-directly.-1" href="#Example-of-using-ChainRules-directly.-1">Example of using ChainRules directly.</a></h2><p>While ChainRules is largely intended as a backend for autodiff systems, it can be used directly. In fact, this can be very useful if you can constrain the code you need to differentiate to only use things that have rules defined for. This was once how all neural network code worked.</p><p>Using ChainRules directly also helps get a feel for it.</p><pre><code class="language-julia">using ChainRules

function foo(x)
    a = sin(x)
    b = 2a
    c = asin(b)
    return c
end

#### Find dfoo/dx via rrules

# First the forward pass, accumulating rules
x = 3;
a, a_pullback = rrule(sin, x);
b, b_pullback = rrule(*, 2, a);
c, c_pullback = rrule(asin, b)

# Then the backward pass calculating gradients
c̄ = 1;  # ∂c/∂c
_, b̄ = c_pullback(extern(c̄));     # ∂c/∂b
_, _, ā = b_pullback(extern(b̄));  # ∂c/∂a
_, x̄ = a_pullback(extern(ā));     # ∂c/∂x = ∂f/∂x
extern(x̄)
# -2.0638950738662625

#### Find dfoo/dx via frules

# Unlike with rrule, we can interleave evaluation and derivative evaluation
x = 3;
ẋ = 1;  # ∂x/∂x
nofields = NamedTuple();  # ∂self/∂self

a, a_pushforward = frule(sin, x);
ȧ = a_pushforward(nofields, extern(ẋ));     # ∂a/∂x

b, b_pushforward = frule(*, 2, a);
ḃ = b_pushforward(nofields, 0, extern(ȧ));  # ∂b/∂x = ∂b/∂a⋅∂a/∂x

c, c_pushforward = frule(asin, b);
ċ = c_pushforward(nofields, extern(ḃ));     # ∂c/∂x = ∂c/∂b⋅∂b/∂x = ∂f/∂x
extern(ċ)
# -2.0638950738662625

#### Find dfoo/dx via finite-differences

using FiniteDifferences
central_fdm(5, 1)(foo, x)
# -2.0638950738670734

#### Find dfoo/dx via finite-differences ForwardDiff.jl
using ForwardDiff
ForwardDiff.derivative(foo, x)
# -2.0638950738662625

#### Find dfoo/dx via finite-differences Zygote.jl
using Zygote
Zygote.gradient(foo, x)
# (-2.0638950738662625,)</code></pre><hr/><h2><a class="nav-anchor" id="On-writing-good-rrule-/-frule-methods-1" href="#On-writing-good-rrule-/-frule-methods-1">On writing good <code>rrule</code> / <code>frule</code> methods</a></h2><h3><a class="nav-anchor" id="Use-Zero()-or-One()-as-return-value-1" href="#Use-Zero()-or-One()-as-return-value-1">Use <code>Zero()</code> or <code>One()</code> as return value</a></h3><p>The <code>Zero()</code> and <code>One()</code> differential objects exist as an alternative to directly returning <code>0</code> or <code>zeros(n)</code>, and <code>1</code> or <code>I</code>. They allow more optimal computation when chaining pullbacks/pushforwards, to avoid work. They should be used where possible.</p><h3><a class="nav-anchor" id="Use-Thunks-appropriately:-1" href="#Use-Thunks-appropriately:-1">Use <code>Thunk</code>s appropriately:</a></h3><p>If work is only required for one of the returned differentials, then it should be wrapped in a <code>@thunk</code> (potentially using a <code>begin</code>-<code>end</code> block).</p><p>If there are multiple return values, their computation should almost always be wrapped in a <code>@thunk</code>.</p><p>Do <em>not</em> wrap <em>variables</em> in a <code>@thunk</code>; wrap the <em>computations</em> that fill those variables in <code>@thunk</code>:</p><pre><code class="language-julia"># good:
∂A = @thunk(foo(x))
return ∂A

# bad:
∂A = foo(x)
return @thunk(∂A)</code></pre><p>In the bad example <code>foo(x)</code> gets computed eagerly, and all that the thunk is doing is wrapping the already calculated result in a function that returns it.</p><h3><a class="nav-anchor" id="Be-careful-with-using-adjoint-when-you-mean-transpose-1" href="#Be-careful-with-using-adjoint-when-you-mean-transpose-1">Be careful with using <code>adjoint</code> when you mean <code>transpose</code></a></h3><p>Remember for complex numbers <code>a&#39;</code> (i.e. <code>adjoint(a)</code>) takes the complex conjugate. Instead you probably want <code>transpose(a)</code>, unless you&#39;ve already restricted <code>a</code> to be a <code>AbstractMatrix{&lt;:Real}</code>.</p><h3><a class="nav-anchor" id="Code-Style-1" href="#Code-Style-1">Code Style</a></h3><p>Use named local functions for the <code>pushforward</code>/<code>pullback</code>:</p><pre><code class="language-julia"># good:
function frule(::typeof(foo), x)
    Y = foo(x)
    function foo_pushforward(_, ẋ)
        return bar(ẋ)
    end
    return Y, foo_pushforward
end
#== output
julia&gt; frule(foo, 2)
(4, var&quot;#foo_pushforward#11&quot;())
==#

# bad:
function frule(::typeof(foo), x)
    return foo(x), (_, ẋ) -&gt; bar(ẋ)
end
#== output:
julia&gt; frule(foo, 2)
(4, var&quot;##9#10&quot;())
==#</code></pre><p>While this is more verbose, it ensures that if an error is thrown during the <code>pullback</code>/<code>pushforward</code> the <a href="https://docs.julialang.org/en/v1/base/base/#Base.gensym"><code>gensym</code></a> name of the local function will include the name you gave it. This makes it a lot simpler to debug from the stacktrace.</p><h3><a class="nav-anchor" id="Write-tests-1" href="#Write-tests-1">Write tests</a></h3><p>There are fairly decent tools for writing tests based on <a href="https://github.com/JuliaDiff/FiniteDifferences.jl">FiniteDifferences.jl</a>. They are in <a href="https://github.com/JuliaDiff/ChainRules.jl/blob/master/test/test_util.jl"><code>tests/test_utils.jl</code></a>. Take a look at existing test and you should see how to do stuff.</p><div class="admonition warning"><div class="admonition-title">Warning</div><div class="admonition-text"><p>Use finite differencing to test derivatives. Don&#39;t use analytical derivations for derivatives in the tests. Those are what you use to define the rules, and so can not be confidently used in the test. If you misread/misunderstood them, then your tests/implementation will have the same mistake.</p></div></div><h3><a class="nav-anchor" id="CAS-systems-are-your-friends.-1" href="#CAS-systems-are-your-friends.-1">CAS systems are your friends.</a></h3><p>It is very easy to check gradients or derivatives with a computer algebra system (CAS) like <a href="https://www.wolframalpha.com/input/?i=gradient+atan2%28x%2Cy%29">WolframAlpha</a>.</p><hr/><h2><a class="nav-anchor" id="FAQ-1" href="#FAQ-1">FAQ</a></h2><h3><a class="nav-anchor" id="What-is-up-with-the-different-symbols?-1" href="#What-is-up-with-the-different-symbols?-1">What is up with the different symbols?</a></h3><ul><li><code>Δx</code> is the input to a propagator, (i.e a <em>seed</em> for a <em>pullback</em>; or a <em>perturbation</em> for a <em>pushforward</em>)</li><li><code>∂x</code> is the output of a propagator</li><li><code>dx</code> could be anything, including a pullback. It really should not show up outside of tests.</li><li><code>v̇</code> is a derivative of the input moving forward: <span>$v̇ = \frac{∂v}{∂x}$</span> for input <span>$x$</span>, intermediate value <span>$v$</span>.</li><li><code>v̄</code> is a derivative of the output moving backward: <span>$v̄ = \frac{∂y}{∂v}$</span> for output <span>$y$</span>, intermediate value <span>$v$</span>.</li><li><code>Ω</code> is often used as the return value of the function. Especially, but not exclusively, for scalar functions.<ul><li><code>ΔΩ</code> is thus a seed for the pullback.</li><li><code>∂Ω</code> is thus the output of a pushforward.</li></ul></li></ul><h3><a class="nav-anchor" id="Why-does-frule-and-rrule-return-the-function-evaluation?-1" href="#Why-does-frule-and-rrule-return-the-function-evaluation?-1">Why does <code>frule</code> and <code>rrule</code> return the function evaluation?</a></h3><p>You might wonder why <code>frule(f, x)</code> returns <code>f(x)</code> and the pushforward for <code>f</code> at <code>x</code>, and similarly for <code>rrule</code> returing <code>f(x)</code> and the pullback for <code>f</code> at <code>x</code>. Why not just return the pushforward/pullback, and let the user call <code>f(x)</code> to get the answer seperately?</p><p>There are two reasons the rules also calculate the <code>f(x)</code>.</p><ol><li>For some rules the output value is used in the definition of its propagator. For example <code>tan</code>.</li><li>For some rules an alternative way of calculating <code>f(x)</code> can give the same answer while also generating intermediate values that can be used in the calculations within the propagator.</li></ol><h3><a class="nav-anchor" id="Where-are-the-gradients-for-keyword-arguments?-1" href="#Where-are-the-gradients-for-keyword-arguments?-1">Where are the gradients for keyword arguments?</a></h3><p><em>pullbacks</em> do not return a gradient for keyword arguments; similarly <em>pushfowards</em> do not accept a pertubation for keyword arguments. This is because in practice functions are very rarely differentiable with respect to keyword arguments. As a rule keyword arguments tend to control side-effects, like logging verbsoity, or to be functionality changing to perform a different operation, e.g. <code>dims=3</code>, and thus not differentiable. To the best of our knowledge no julia AD system, with support for the definition of custom primatives, supports differentating with respect to keyword arguments. At some point in the future ChainRules may support these. Maybe.</p><footer><hr/><a class="next" href="getting_started.html"><span class="direction">Next</span><span class="title">Getting Started</span></a></footer></article></body></html>
