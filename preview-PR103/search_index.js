var documenterSearchIndex = {"docs":
[{"location":"api.html#API-Documentation-1","page":"API","title":"API Documentation","text":"","category":"section"},{"location":"api.html#","page":"API","title":"API","text":"Modules = [ChainRulesCore]","category":"page"},{"location":"api.html#ChainRulesCore.NO_FIELDS","page":"API","title":"ChainRulesCore.NO_FIELDS","text":"NO_FIELDS\n\nConstant for the reverse-mode derivative with respect to a structure that has no fields. The most notable use for this is for the reverse-mode derivative with respect to the function itself, when that function is not a closure.\n\n\n\n\n\n","category":"constant"},{"location":"api.html#ChainRulesCore.Casted","page":"API","title":"ChainRulesCore.Casted","text":"Casted(v)\n\nThis differential wraps another differential (including a number-like type) to indicate that it should be lazily broadcast.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.DNE","page":"API","title":"ChainRulesCore.DNE","text":"DNE()\n\nThis differential indicates that the derivative Does Not Exist (D.N.E). This is not the cast that it is not implemented, but rather that it mathematically is not defined.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.InplaceableThunk","page":"API","title":"ChainRulesCore.InplaceableThunk","text":"InplaceableThunk(val::Thunk, add!::Function)\n\nA wrapper for a Thunk, that allows it to define an inplace add! function, which is used internally in accumulate!(Δ, ::InplaceableThunk).\n\nadd! should be defined such that: ithunk.add!(Δ) = Δ .+= ithunk.val but it should do this more efficently than simply doing this directly. (Otherwise one can just use a normal Thunk).\n\nMost operations on an InplaceableThunk treat it just like a normal Thunk; and destroy its inplacability.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.One","page":"API","title":"ChainRulesCore.One","text":" One()\n\nThe Differential which is the multiplicative identity. Basically, this represents 1.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.Thunk","page":"API","title":"ChainRulesCore.Thunk","text":"Thunk(()->v)\n\nA thunk is a deferred computation. It wraps a zero argument closure that when invoked returns a differential. @thunk(v) is a macro that expands into Thunk(()->v).\n\nCalling a thunk, calls the wrapped closure. externing thunks applies recursively, it also externs the differial that the closure returns. If you do not want that, then simply call the thunk\n\njulia> t = @thunk(@thunk(3))\nThunk(var\"##7#9\"())\n\njulia> extern(t)\n3\n\njulia> t()\nThunk(var\"##8#10\"())\n\njulia> t()()\n3\n\nWhen to @thunk?\n\nWhen writing rrules (and to a lesser exent frules), it is important to @thunk appropriately. Propagation rule's that return multiple derivatives are not able to do all the computing themselves.  By @thunking the work required for each, they then compute only what is needed.\n\nSo why not thunk everything?\n\n@thunk creates a closure over the expression, which (effectively) creates a struct with a field for each variable used in the expression, and call overloaded.\n\nDo not use @thunk if this would be equal or more work than actually evaluating the expression itself. Examples being:\n\nThe expression wrapping something in a struct, such as Adjoint(x) or Diagonal(x)\nThe expression being a constant\nThe expression being itself a thunk\nThe expression being from another rrule or frule (it would be @thunked if required by the defining rule already)\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.Wirtinger","page":"API","title":"ChainRulesCore.Wirtinger","text":"Wirtinger(primal::Union{Number,AbstractDifferential},\n          conjugate::Union{Number,AbstractDifferential})\n\nReturns a Wirtinger instance representing the complex differential:\n\ndf = ∂f/∂z * dz + ∂f/∂z̄ * dz̄\n\nwhere primal corresponds to ∂f/∂z * dz and conjugate corresponds to ∂f/∂z̄ * dz̄.\n\nThe two fields of the returned instance can be accessed generically via the wirtinger_primal and wirtinger_conjugate methods.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.Zero","page":"API","title":"ChainRulesCore.Zero","text":"Zero()\n\nThe additive identity for differentials. This is basically the same as 0.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.extern-Tuple{Any}","page":"API","title":"ChainRulesCore.extern","text":"extern(x)\n\nReturn x converted to an appropriate non-AbstractDifferential type, for use with external packages that might not handle AbstractDifferential types.\n\nNote that this function may return an alias (not necessarily a copy) to data wrapped by x, such that mutating extern(x) might mutate x itself.\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.frule-Tuple{Any,Vararg{Any,N} where N}","page":"API","title":"ChainRulesCore.frule","text":"frule(f, x...)\n\nExpressing x as the tuple (x₁, x₂, ...) and the output tuple of f(x...) as Ω, return the tuple:\n\n(Ω, (ṡelf, ẋ₁, ẋ₂, ...) -> Ω̇₁, Ω̇₂, ...)\n\nThe second return value is the propagation rule, or the pushforward. It takes in differentials corresponding to the inputs (ẋ₁, ẋ₂, ...) and ṡelf the internal values of the function (for closures).\n\nIf no method matching frule(f, xs...) has been defined, then return nothing.\n\nExamples:\n\nunary input, unary output scalar function:\n\njulia> x = rand();\n\njulia> sinx, sin_pushforward = frule(sin, x);\n\njulia> sinx == sin(x)\ntrue\n\njulia> sin_pushforward(NamedTuple(), 1) == cos(x)\ntrue\n\nunary input, binary output scalar function:\n\njulia> x = rand();\n\njulia> sincosx, sincos_pushforward = frule(sincos, x);\n\njulia> sincosx == sincos(x)\ntrue\n\njulia> sincos_pushforward(NamedTuple(), 1) == (cos(x), -sin(x))\ntrue\n\nSee also: rrule, @scalar_rule\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.refine_differential-Tuple{Type{#s15} where #s15<:(Union{AbstractArray{#s17,N} where N where #s17<:Real, #s16} where #s16<:Real),Wirtinger}","page":"API","title":"ChainRulesCore.refine_differential","text":"refine_differential(𝒟::Type, der)\n\nConverts, if required, a differential object der (e.g. a Number, AbstractDifferential, Matrix, etc.), to another  differential that is more suited for the domain given by the type 𝒟. Often this will behave as the identity function on der.\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.rrule-Tuple{Any,Vararg{Any,N} where N}","page":"API","title":"ChainRulesCore.rrule","text":"rrule(f, x...)\n\nExpressing x as the tuple (x₁, x₂, ...) and the output tuple of f(x...) as Ω, return the tuple:\n\n(Ω, (Ω̄₁, Ω̄₂, ...) -> (s̄elf, x̄₁, x̄₂, ...))\n\nWhere the second return value is the the propagation rule or pullback. It takes in differentials corresponding to the outputs (x̄₁, x̄₂, ...), and s̄elf, the internal values of the function itself (for closures)\n\nIf no method matching rrule(f, xs...) has been defined, then return nothing.\n\nExamples:\n\nunary input, unary output scalar function:\n\njulia> x = rand();\n\njulia> sinx, sin_pullback = rrule(sin, x);\n\njulia> sinx == sin(x)\ntrue\n\njulia> sin_pullback(1) == (NO_FIELDS, cos(x))\ntrue\n\nbinary input, unary output scalar function:\n\njulia> x, y = rand(2);\n\njulia> hypotxy, hypot_pullback = rrule(hypot, x, y);\n\njulia> hypotxy == hypot(x, y)\ntrue\n\njulia> hypot_pullback(1) == (NO_FIELDS, (x / hypot(x, y)), (y / hypot(x, y)))\ntrue\n\nSee also: frule, @scalar_rule\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.store!-Tuple{Any,Any}","page":"API","title":"ChainRulesCore.store!","text":"store!(Δ, ∂)\n\nStores ∂, in Δ, overwriting what ever was in Δ before. potentially avoiding intermediate temporary allocations that might be necessary for alternative approaches  (e.g. copyto!(Δ, extern(∂)))\n\nLike accumulate and accumulate!, this function is intended to be customizable for specific rules/input types.\n\nSee also: accumulate, accumulate!, AbstractRule\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.@scalar_rule-Tuple{Any,Any,Vararg{Any,N} where N}","page":"API","title":"ChainRulesCore.@scalar_rule","text":"@scalar_rule(f(x₁, x₂, ...),\n             @setup(statement₁, statement₂, ...),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...)\n\nA convenience macro that generates simple scalar forward or reverse rules using the provided partial derivatives. Specifically, generates the corresponding methods for frule and rrule:\n\nfunction ChainRulesCore.frule(::typeof(f), x₁::Number, x₂::Number, ...)\n    Ω = f(x₁, x₂, ...)\n    $(statement₁, statement₂, ...)\n    return Ω, (_, Δx₁, Δx₂, ...) -> (\n            (∂f₁_∂x₁ * Δx₁ + ∂f₁_∂x₂ * Δx₂ + ...),\n            (∂f₂_∂x₁ * Δx₁ + ∂f₂_∂x₂ * Δx₂ + ...),\n            ...\n        )\nend\n\nfunction ChainRulesCore.rrule(::typeof(f), x₁::Number, x₂::Number, ...)\n    Ω = f(x₁, x₂, ...)\n    $(statement₁, statement₂, ...)\n    return Ω, (ΔΩ₁, ΔΩ₂, ...) -> (\n            NO_FIELDS,\n            ∂f₁_∂x₁ * ΔΩ₁ + ∂f₂_∂x₁ * ΔΩ₂ + ...),\n            ∂f₁_∂x₂ * ΔΩ₁ + ∂f₂_∂x₂ * ΔΩ₂ + ...),\n            ...\n        )\nend\n\nIf no type constraints in f(x₁, x₂, ...) within the call to @scalar_rule are provided, each parameter in the resulting frule/rrule definition is given a type constraint of Number. Constraints may also be explicitly be provided to override the Number constraint, e.g. f(x₁::Complex, x₂), which will constrain x₁ to Complex and x₂ to Number.\n\nAt present this does not support defining for closures/functors. Thus in reverse-mode, the first returned partial, representing the derivative with respect to the function itself, is always NO_FIELDS. And in forward-mode, the first input to the returned propagator is always ignored.\n\nThe result of f(x₁, x₂, ...) is automatically bound to Ω. This allows the primal result to be conveniently referenced (as Ω) within the derivative/setup expressions.\n\nThe @setup argument can be elided if no setup code is need. In other words:\n\n@scalar_rule(f(x₁, x₂, ...),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...)\n\nis equivalent to:\n\n@scalar_rule(f(x₁, x₂, ...),\n             @setup(nothing),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...)\n\nFor examples, see ChainRulesCore' rules directory.\n\nSee also: frule, rrule, AbstractRule\n\n\n\n\n\n","category":"macro"},{"location":"api.html#ChainRulesCore.AbstractDifferential","page":"API","title":"ChainRulesCore.AbstractDifferential","text":"The subtypes of AbstractDifferential define a custom \"algebra\" for chain rule evaluation that attempts to factor various features like complex derivative support, broadcast fusion, zero-elision, etc. into nicely separated parts.\n\nAll subtypes of AbstractDifferential implement the following operations:\n\n+(a, b): linearly combine differential a and differential b\n\n*(a, b): multiply the differential a by the differential b\n\nBase.conj(x): complex conjugate of the differential x\n\nextern(x): convert x into an appropriate non-AbstractDifferential type for use outside of ChainContext.\n\nValid arguments to these operations are T where T<:AbstractDifferential, or where T has proper + and * implementations.\n\nAdditionally, all subtypes of AbstractDifferential support Base.iterate and Base.Broadcast.broadcastable(x).\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore._normalize_scalarrules_macro_input-Tuple{Any,Any,Any}","page":"API","title":"ChainRulesCore._normalize_scalarrules_macro_input","text":"_normalize_scalarrules_macro_input(call, maybe_setup, partials)\n\nreturns (in order) the correctly escaped:     - call with out any type constraints     - setup_stmts: the content of @setup or nothing if that is not provided,     -  inputs: with all args having the constraints removed from call, or         defaulting to Number     - partials: which are all Expr{:tuple,...}\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.accumulate!-Tuple{Any,Any}","page":"API","title":"ChainRulesCore.accumulate!","text":"accumulate!(Δ, ∂)\n\nSimilar to accumulate, but attempts to compute Δ + rule(args...) in-place, storing the result in Δ.\n\nNote: this function may not actually store the result in Δ if Δ is immutable, so it is best to always call this as Δ = accumulate!(Δ, ∂) just in-case.\n\nThis function is overloadable by using a InplaceThunk. See also: accumulate, store!.\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.accumulate-Tuple{Any,Any}","page":"API","title":"ChainRulesCore.accumulate","text":"accumulate(Δ, ∂)\n\nReturn Δ + ∂ evaluated in a manner that supports ChainRulesCore's various AbstractDifferential types.\n\nSee also: accumulate!, store!, AbstractRule\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.propagation_expr-Tuple{Any,Any,Any}","page":"API","title":"ChainRulesCore.propagation_expr","text":"propagation_expr(𝒟, Δs, ∂s)\n\nReturns the expression for the propagation of\nthe input gradient `Δs` though the partials `∂s`.\n\n𝒟 is an expression that when evaluated returns the type-of the input domain.\nFor example if the derivative is being taken at the point `1` it returns `Int`.\nif it is taken at `1+1im` it returns `Complex{Int}`.\nAt present it is ignored for non-Wirtinger derivatives.\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.propagator_name-Tuple{Expr,Symbol}","page":"API","title":"ChainRulesCore.propagator_name","text":"propagator_name(f, propname)\n\nDetermines a reasonable name for the propagator function. The name doesn't really matter too much as it is a local function to be returned by frule or rrule, but a good name make debugging easier. f should be some form of AST representation of the actual function, propname should be either :pullback or :pushforward\n\nThis is able to deal with fairly complex expressions for f:\n\njulia> propagator_name(:bar, :pushforward)\n:bar_pushforward\n\njulia> propagator_name(esc(:(Base.Random.foo)), :pullback)\n:foo_pullback\n\n\n\n\n\n","category":"method"},{"location":"getting_started.html#Getting-Started-1","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started.html#","page":"Getting Started","title":"Getting Started","text":"ChainRulesCore.jl is a light-weight dependency for defining sensitivities for functions in your packages, without you needing to depend on ChainRules itself. It has no dependencies of its own.","category":"page"},{"location":"getting_started.html#","page":"Getting Started","title":"Getting Started","text":"ChainRules.jl provides the full functionality, including sensitivities for Base Julia and standard libraries. Sensitivities for some other packages, currently SpecialFunctions.jl and NaNMath.jl, will also be loaded if those packages are in your environment. In general, we recommend adding custom sensitivities to your own packages with ChainRulesCore, rather than adding them to ChainRules.jl.","category":"page"},{"location":"getting_started.html#Defining-Custom-Sensitivities-1","page":"Getting Started","title":"Defining Custom Sensitivities","text":"","category":"section"},{"location":"getting_started.html#","page":"Getting Started","title":"Getting Started","text":"TODO","category":"page"},{"location":"getting_started.html#Forward-Mode-vs.-Reverse-Mode-Chain-Rule-Evaluation-1","page":"Getting Started","title":"Forward-Mode vs. Reverse-Mode Chain Rule Evaluation","text":"","category":"section"},{"location":"getting_started.html#","page":"Getting Started","title":"Getting Started","text":"TODO","category":"page"},{"location":"getting_started.html#Real-Scalar-Differentiation-Rules-1","page":"Getting Started","title":"Real Scalar Differentiation Rules","text":"","category":"section"},{"location":"getting_started.html#","page":"Getting Started","title":"Getting Started","text":"TODO","category":"page"},{"location":"getting_started.html#Complex-Scalar-Differentiation-Rules-1","page":"Getting Started","title":"Complex Scalar Differentiation Rules","text":"","category":"section"},{"location":"getting_started.html#","page":"Getting Started","title":"Getting Started","text":"TODO","category":"page"},{"location":"getting_started.html#Non-Scalar-Differentiation-Rules-1","page":"Getting Started","title":"Non-Scalar Differentiation Rules","text":"","category":"section"},{"location":"getting_started.html#","page":"Getting Started","title":"Getting Started","text":"TODO","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"DocTestSetup = :(using ChainRulesCore, ChainRules)","category":"page"},{"location":"index.html#ChainRules-1","page":"Introduction","title":"ChainRules","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"ChainRules provides a variety of common utilities that can be used by downstream automatic differentiation (AD) tools to define and execute forward-, reverse-, and mixed-mode primitives.","category":"page"},{"location":"index.html#Introduction-1","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"ChainRules is all about providing a rich set of rules for differentiation. When a person learns introductory calculus, they learn that the derivative (with respect to x) of a*x is a, and the derivative of sin(x) is cos(x), etc. And they learn how to combine simple rules, via the chain rule, to differentiate complicated functions. ChainRules is a programmatic repository of that knowledge, with the generalizations to higher dimensions.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Autodiff (AD) tools roughly work by reducing a problem down to simple parts that they know the rules for, and then combining those rules. Knowing rules for more complicated functions speeds up the autodiff process as it doesn't have to break things down as much.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"ChainRules is an AD-independent collection of rules to use in a differentiation system.","category":"page"},{"location":"index.html#frule-and-rrule-1","page":"Introduction","title":"frule and rrule","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"terminology: `frule` and `rrule`\nfrule and rrule are ChainRules specific terms. Their exact functioning is fairly ChainRules specific, though other tools have similar functions. The core notion is sometimes called custom AD primitives, custom adjoints, customgradients, _custom sensitivities. (Potentially incorrectly, terminology is often abused.)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The rules are encoded as frules and rrules, for use in forward-mode and reverse-mode differentiation respectively.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Similarly, the frule is written:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"function frule(::typeof(foo), args; kwargs...)\n    ...\n    return y, pushforward\nend","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"where y = foo(args; kwargs...), and pushforward is a function to propagate the derivative information forwards at that point (more later).","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The rrule for some function foo, which takes the positional argument args and keyword argument kwargs, is written:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"function rrule(::typeof(foo), args; kwargs...)\n    ...\n    return y, pullback\nend","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"again y must be equal to foo(args; kwargs...), and pullback is a function to propagate the derivative information backwards at that point (more later).","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Almost always the pushforward/pullback will be declared locally within the frule/rrule, and will be a closure over some of the other arguments.","category":"page"},{"location":"index.html#The-propagators:-pushforward-and-pullback-1","page":"Introduction","title":"The propagators: pushforward and pullback","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"terminology: pushforward and pullback\nPushforward and pullback are fancy words that the autodiff community adopted from Differential Geometry. The are broadly in agreement with the use of pullback and pushforward in differential geometry. But any geometer will tell you these are the super-boring flat cases. Some will also frown at you. Other terms that may be used include for pullback the backpropagator, and by analogy for pushforward the forwardpropagator, thus these are the propagators. These are also good names because effectively they propagate wiggles and wobbles through them, via the chain rule. (the term backpropagator may originate with \"Lambda The Ultimate Backpropagator\" by Pearlmutter and Siskind, 2008)","category":"page"},{"location":"index.html#Core-Idea-1","page":"Introduction","title":"Core Idea","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"<!––-","category":"page"},{"location":"index.html#TODO:-Incorperate-this:-1","page":"Introduction","title":"TODO: Incorperate this:","text":"","category":"section"},{"location":"index.html#wesselb-9-days-ago-Member-1","page":"Introduction","title":"wesselb 9 days ago Member","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Are these ideas consistent with what pushforward and pullback do? I'm not familiar with ChainRules and its internals, but I anticipated pushforward and pullback to do the following: Consider a computation x -> u -> f(u) = v -> y. Then pushforward for f turns du/dx into dv/dx, whereas pullback turns dy/dv into dy/du. So pushforward pushes a \"sensitivity with respect to the input through the function\", whereas pullback pulls a \"sensitivity with respect to the output back through the function\". Perhaps that's what the below convey, not sure... maybe I'm just rambling.","category":"page"},{"location":"index.html#@jekbradbury-1","page":"Introduction","title":"@jekbradbury","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Yeah, I think the below is accurate for the pushforward but misleading for the pullback. The pullback doesn’t take an output wobble and produce an input wiggle (that would be left-multiplying by the inverse of the Jacobian); it takes an output sensitivity (“how much does the loss function wobble when you wiggle the output”) and produces an input sensitivity (“how much does the loss function wobble when you wiggle the input”). This corresponds to left-multiplying by the adjoint of the Jacobian—an important distinction!","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"If the output is the scalar loss and you call the pullback on the scalar 1, then it will produce the gradient of the input (also a vector in the cotangent space, aka a wobble-wiggle ratio).","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"This is still misleading for the pullback. Reposting a comment that got lost: The pullback doesn’t take an output wobble and produce an input wiggle (that would be left-multiplying by the inverse of the Jacobian); it takes an output sensitivity (“how much does the loss function wobble when you wiggle the output”) and produces an input sensitivity (“how much does the loss function wobble when you wiggle the input”). This corresponds to left-multiplying by the adjoint of the Jacobian—an important distinction!","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"If the output is the scalar loss and you call the pullback on the scalar 1, then it will produce the gradient of the input (also a vector in the cotangent space, aka a wobble-wiggle ratio).","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"–––––>","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The pushforward takes a wiggle in the input space, and tells what wobble you would create in the output space, by passing it through the function.\nThe pullback takes wobblyness information with respect to the function's output, and tells the equivalent wobblyness with repect to the functions input.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Definitions:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"wobblyness: a sensitivity\nwobble: a differential in the output space\nwiggle: a differential in the input space","category":"page"},{"location":"index.html#Math-1","page":"Introduction","title":"Math","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"If I have some functions: g(a), h(b) and f(x)=g(h(x)), ∂ and I know the pullback of g, at h(x) written: mathrmpullback_g(a)a=h(x),","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"and I know the deriviative of h with respect to its input b at g(x), written: leftdfractexthtextbright_b=g(x)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Then I can use the pullback to find: dfractextftextx","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"dfractextftextx=mathrmmathrmpullback_g(a)a=h(x)left(leftdfractexthtextbright_b=g(x)right)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"—","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"If I know the deriviative of g with respect to its input a at x, written: leftdfractextgtextaright_a=x","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"and I know the pushforward of h at g(x) written: mathrmpushforward_h(b)b=g(x)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"then I can use the pushforward to find dfractextftextx","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"dfractextftextx=mathrmpushforward_h(b)b=g(x)left(leftdfractextgtextaright_a=xright)","category":"page"},{"location":"index.html#The-anatomy-of-pushforward-and-pullback-1","page":"Introduction","title":"The anatomy of pushforward and pullback","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"For our function foo(args...; kwargs) = Y:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The pushforward is a function:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"function pushforward(Δself, Δargs...)\n    ...\n    return ∂Y\nend","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The input to the pushforward is often called the perturbation. If the function is y = f(x) often the pushforward will be written ẏ = pushforward(ṡelf, ẋ). (ẏ is commonly used to represent the pertubation for y)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"note: Note\nThere is one Δarg per arg to the original function. The Δargs are similar in type/structure to the corresponding inputs args (Δself is explained below). The ∂Y are similar in type/structure to the original function's output Y. In particular if that function returned a tuple then ∂Y will be a tuple of same size.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The pullback is a function:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"function pullback(ΔY)\n    ...\n    return ∂self, ∂args...\nend","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The input to the pullback is often called the seed. If the function is y = f(x) often the pullback will be written s̄elf, x̄ = pullback(ȳ).","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"note: Note\nThe pullback returns one ∂arg per arg to the original function, plus one for the fields of the function itself (explained below).","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"terminology: Terminology\nSometimes perturbation, seed, and sensitivity will be used interchangeably, depending on task/subfield (sensitivity analysis and perturbation theory are apparently very big on just calling everything sensitivity or perturbation respectively.) At the end of the day, they are all wiggles or wobbles.","category":"page"},{"location":"index.html#Self-derivative-Δself,-self,-self,-self-etc.-1","page":"Introduction","title":"Self derivative Δself, ∂self, s̄elf, ṡelf etc.","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"!!! terminology  Δself, ∂self, s̄elf, ṡelf     It is the derivatives with respect to the internal fields of the function.     To the best of our knowledge there is no standard terminology for this.     Other good names might be Δinternal/∂internal.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"From the mathematical perspective, one may have been wondering what all this Δself, ∂self is. Given that a function with two inputs, say f(a, b), only has two partial derivatives: dfracfa, dfracfb. Why then does a pushforward take in this extra Δself, and why does a pullback return this extra ∂self?","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The reason is that in Julia the function f may itself have internal fields. For example a closure has the fields it closes over; a callable object (i.e. a functor) like a Flux.Dense has the fields of that object.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Thus every function is treated as having the extra implicit argument self, which captures those fields. So every pushforward takes in an extra argument, which is ignored unless the original function has fields. It is common to write function foo_pushforward(_, Δargs...) in the case when foo does not have fields. Similarly every pullback returns an extra ∂self, which for things without fields is the constant NO_FIELDS, indicating there are no fields within the function itself.","category":"page"},{"location":"index.html#Pushforward-/-Pullback-summary-1","page":"Introduction","title":"Pushforward / Pullback summary","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Pushforward:\nreturned by frule\ntakes input space wiggles, gives output space wobbles\n1 argument per original function argument + 1 for the function itself\n1 return per original function return\nPullback\nreturned by rrule\ntakes output space wobbles, gives input space wiggles\n1 argument per original function return\n1 return per original function argument + 1 for the function itself","category":"page"},{"location":"index.html#Pushforward/Pullback-and-Total-Derivative/Gradient-1","page":"Introduction","title":"Pushforward/Pullback and Total Derivative/Gradient","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The most trivial use of frule and returned pushforward is to calculate the Total Derivative:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"y, f_pushforward = frule(f, a, b, c)\nẏ = f_pushforward(1, 1, 1, 1)  # for appropriate `1`-like perturbation.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Then we have that ẏ is the total derivative of f at (a, b, c), written mathematically as df_(abc)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Similarly, the most trivial use of rrule and returned pullback is to calculate the Gradient:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"y, f_pullback = rrule(f, a, b, c)\n∇f = f_pullback(1)  # for appropriate `1`-like seed.\ns̄elf, ā, b̄, c̄ = ∇f","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Then we have that ∇f is the gradient of f at (a, b, c). And we thus have the partial derivatives overlinemathrmself = dfracfmathrmself, overlinea = dfracfa, overlineb = dfracfb, overlinec = dfracfc, including the and the self-partial derivative, overlinemathrmself.","category":"page"},{"location":"index.html#Differentials-1","page":"Introduction","title":"Differentials","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The values that come back from pullbacks or pushforwards are not always the same type as the input/outputs of the original function. They are differentials, which correspond roughly to something able to represent the difference between two values of the original types. A differential might be such a regular type, like a Number, or a Matrix, matching to the original type; or it might be one of the AbstractDifferential subtypes.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Differentials support a number of operations. Most importantly: + and *, which let them act as mathematical objects. And extern which converts AbstractDifferential types into a conventional non-ChainRules type.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The most important AbstractDifferentials when getting started are the ones about avoiding work:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Thunk: this is a deferred computation. A thunk is a word for a zero argument closure. A computation wrapped in a @thunk doesn't get evaluated until extern is called on the Thunk. More on thunks later.\nOne, Zero: There are special representations of 1 and 0. They do great things around avoiding expanding Thunks in multiplication and (for Zero) addition.","category":"page"},{"location":"index.html#Other-AbstractDifferentials:-don't-worry-about-them-right-now-1","page":"Introduction","title":"Other AbstractDifferentials: don't worry about them right now","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Wirtinger: it is complex. The docs need to be better. Read the links in this issue.\nCasted: it implements broadcasting mechanics. See #10\nInplaceableThunk: it is like a Thunk but it can do store! and accumulate! in-place.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"index.html#Example-of-using-ChainRules-directly.-1","page":"Introduction","title":"Example of using ChainRules directly.","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"While ChainRules is largely intended as a backend for autodiff systems, it can be used directly. In fact, this can be very useful if you can constrain the code you need to differentiate to only use things that have rules defined for. This was once how all neural network code worked.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Using ChainRules directly also helps get a feel for it.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"using ChainRules\n\nfunction foo(x)\n    a = sin(x)\n    b = 2a\n    c = asin(b)\n    return c\nend\n\n#### Find dfoo/dx via rrules\n\n# First the forward pass, accumulating rules\nx = 3;\na, a_pullback = rrule(sin, x);\nb, b_pullback = rrule(*, 2, a);\nc, c_pullback = rrule(asin, b)\n\n# Then the backward pass calculating gradients\nc̄ = 1;  # ∂c/∂c\n_, b̄ = c_pullback(extern(c̄));     # ∂c/∂b\n_, _, ā = b_pullback(extern(b̄));  # ∂c/∂a\n_, x̄ = a_pullback(extern(ā));     # ∂c/∂x = ∂f/∂x\nextern(x̄)\n# -2.0638950738662625\n\n#### Find dfoo/dx via frules\n\n# Unlike with rrule, we can interleave evaluation and derivative evaluation\nx = 3;\nẋ = 1;  # ∂x/∂x\nnofields = NamedTuple();  # ∂self/∂self\n\na, a_pushforward = frule(sin, x);\nȧ = a_pushforward(nofields, extern(ẋ));     # ∂a/∂x\n\nb, b_pushforward = frule(*, 2, a);\nḃ = b_pushforward(nofields, 0, extern(ȧ));  # ∂b/∂x = ∂b/∂a⋅∂a/∂x\n\nc, c_pushforward = frule(asin, b);\nċ = c_pushforward(nofields, extern(ḃ));     # ∂c/∂x = ∂c/∂b⋅∂b/∂x = ∂f/∂x\nextern(ċ)\n# -2.0638950738662625\n\n#### Find dfoo/dx via finite-differences\n\nusing FiniteDifferences\ncentral_fdm(5, 1)(foo, x)\n# -2.0638950738670734\n\n#### Find dfoo/dx via ForwardDiff.jl\nusing ForwardDiff\nForwardDiff.derivative(foo, x)\n# -2.0638950738662625\n\n#### Find dfoo/dx via Zygote.jl\nusing Zygote\nZygote.gradient(foo, x)\n# (-2.0638950738662625,)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"index.html#On-writing-good-rrule-/-frule-methods-1","page":"Introduction","title":"On writing good rrule / frule methods","text":"","category":"section"},{"location":"index.html#Use-Zero()-or-One()-as-return-value-1","page":"Introduction","title":"Use Zero() or One() as return value","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"The Zero() and One() differential objects exist as an alternative to directly returning 0 or zeros(n), and 1 or I. They allow more optimal computation when chaining pullbacks/pushforwards, to avoid work. They should be used where possible.","category":"page"},{"location":"index.html#Use-Thunks-appropriately:-1","page":"Introduction","title":"Use Thunks appropriately:","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"If work is only required for one of the returned differentials, then it should be wrapped in a @thunk (potentially using a begin-end block).","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"If there are multiple return values, their computation should almost always be wrapped in a @thunk.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Do not wrap variables in a @thunk; wrap the computations that fill those variables in @thunk:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"# good:\n∂A = @thunk(foo(x))\nreturn ∂A\n\n# bad:\n∂A = foo(x)\nreturn @thunk(∂A)","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"In the bad example foo(x) gets computed eagerly, and all that the thunk is doing is wrapping the already calculated result in a function that returns it.","category":"page"},{"location":"index.html#Be-careful-with-using-adjoint-when-you-mean-transpose-1","page":"Introduction","title":"Be careful with using adjoint when you mean transpose","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Remember for complex numbers a' (i.e. adjoint(a)) takes the complex conjugate. Instead you probably want transpose(a), unless you've already restricted a to be a AbstractMatrix{<:Real}.","category":"page"},{"location":"index.html#Code-Style-1","page":"Introduction","title":"Code Style","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Use named local functions for the pushforward/pullback:","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"# good:\nfunction frule(::typeof(foo), x)\n    Y = foo(x)\n    function foo_pushforward(_, ẋ)\n        return bar(ẋ)\n    end\n    return Y, foo_pushforward\nend\n#== output\njulia> frule(foo, 2)\n(4, var\"#foo_pushforward#11\"())\n==#\n\n# bad:\nfunction frule(::typeof(foo), x)\n    return foo(x), (_, ẋ) -> bar(ẋ)\nend\n#== output:\njulia> frule(foo, 2)\n(4, var\"##9#10\"())\n==#","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"While this is more verbose, it ensures that if an error is thrown during the pullback/pushforward the gensym name of the local function will include the name you gave it. This makes it a lot simpler to debug from the stacktrace.","category":"page"},{"location":"index.html#Write-tests-1","page":"Introduction","title":"Write tests","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"There are fairly decent tools for writing tests based on FiniteDifferences.jl. They are in tests/test_utils.jl. Take a look at existing test and you should see how to do stuff.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"warning: Warning\nUse finite differencing to test derivatives. Don't use analytical derivations for derivatives in the tests. Those are what you use to define the rules, and so can not be confidently used in the test. If you misread/misunderstood them, then your tests/implementation will have the same mistake.","category":"page"},{"location":"index.html#CAS-systems-are-your-friends.-1","page":"Introduction","title":"CAS systems are your friends.","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"It is very easy to check gradients or derivatives with a computer algebra system (CAS) like WolframAlpha.","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"index.html#FAQ-1","page":"Introduction","title":"FAQ","text":"","category":"section"},{"location":"index.html#What-is-up-with-the-different-symbols?-1","page":"Introduction","title":"What is up with the different symbols?","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"Δx is the input to a propagator, (i.e a seed for a pullback; or a perturbation for a pushforward)\n∂x is the output of a propagator\ndx could be anything, including a pullback. It really should not show up outside of tests.\nv̇ is a derivative of the input moving forward: v = fracvx for input x, intermediate value v.\nv̄ is a derivative of the output moving backward: v = fracyv for output y, intermediate value v.\nΩ is often used as the return value of the function. Especially, but not exclusively, for scalar functions.\nΔΩ is thus a seed for the pullback.\n∂Ω is thus the output of a pushforward.","category":"page"},{"location":"index.html#Why-does-frule-and-rrule-return-the-function-evaluation?-1","page":"Introduction","title":"Why does frule and rrule return the function evaluation?","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"You might wonder why frule(f, x) returns f(x) and the pushforward for f at x, and similarly for rrule returing f(x) and the pullback for f at x. Why not just return the pushforward/pullback, and let the user call f(x) to get the answer seperately?","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"There are two reasons the rules also calculate the f(x).","category":"page"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"For some rules the output value is used in the definition of its propagator. For example tan.\nFor some rules an alternative way of calculating f(x) can give the same answer while also generating intermediate values that can be used in the calculations within the propagator.","category":"page"},{"location":"index.html#Where-are-the-gradients-for-keyword-arguments?-1","page":"Introduction","title":"Where are the gradients for keyword arguments?","text":"","category":"section"},{"location":"index.html#","page":"Introduction","title":"Introduction","text":"pullbacks do not return a gradient for keyword arguments; similarly pushfowards do not accept a pertubation for keyword arguments. This is because in practice functions are very rarely differentiable with respect to keyword arguments. As a rule keyword arguments tend to control side-effects, like logging verbsoity, or to be functionality changing to perform a different operation, e.g. dims=3, and thus not differentiable. To the best of our knowledge no julia AD system, with support for the definition of custom primatives, supports differentating with respect to keyword arguments. At some point in the future ChainRules may support these. Maybe.","category":"page"}]
}
